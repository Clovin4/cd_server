# network.yml — Automates KtHW Doc 03: Compute Resources
#
# Requires the following host vars in the inventory (set by the CI inventory builder):
#   private_ip   — VPC-internal IP (used in /etc/hosts, machines.txt, ssh config)
#   pod_subnet   — Pod CIDR for worker nodes only (e.g. 10.200.0.0/24)
#
# Run after setup.yml and jumpbox.yml. At this point:
#   - All nodes have deploy user with passwordless sudo
#   - Jumpbox has id_ed25519 and can reach all other nodes
#   - Jumpbox has KtHW repo cloned at /home/deploy/kubernetes-the-hard-way

# ── Play 1: Set hostnames and disable cloud-init /etc/hosts management ────────
# Runs on every node. DigitalOcean's cloud-init rewrites /etc/hosts on each boot
# if manage_etc_hosts is true — we need to turn that off before distributing our
# own hosts file, otherwise our changes will be clobbered on the next reboot.
- name: Configure hostnames
  hosts: all
  become: true
  gather_facts: true

  vars:
    domain: kubernetes.local

  tasks:
    - name: Set hostname to short name
      hostname:
        name: "{{ inventory_hostname }}"

    - name: Disable cloud-init /etc/hosts management
      lineinfile:
        path: /etc/cloud/cloud.cfg
        regexp: "^manage_etc_hosts"
        line: "manage_etc_hosts: false"
        create: false
      failed_when: false  # File may not exist on all images

    - name: Update 127.0.1.1 line in /etc/hosts to use FQDN
      lineinfile:
        path: /etc/hosts
        regexp: "^127\\.0\\.1\\.1"
        line: "127.0.1.1\t{{ inventory_hostname }}.{{ domain }} {{ inventory_hostname }}"

    - name: Verify hostname --fqdn
      command: hostname --fqdn
      register: fqdn_check
      changed_when: false
      failed_when: false

    - name: Print FQDN
      debug:
        msg: "{{ inventory_hostname }}: {{ fqdn_check.stdout }}"


# ── Play 2: Build and distribute /etc/hosts ───────────────────────────────────
# Ansible knows all private_ip host vars from the inventory.
# We build the hosts file content here and push it to every node — no manual
# SCP loops required.
- name: Distribute /etc/hosts
  hosts: all
  become: true
  gather_facts: false

  vars:
    domain: kubernetes.local

  tasks:
    - name: Write /etc/hosts with all cluster nodes
      copy:
        dest: /etc/hosts
        mode: "0644"
        content: |
          # Static table lookup for hostnames.
          # Generated by Ansible — do not edit manually.
          127.0.0.1       localhost
          127.0.1.1       {{ inventory_hostname }}.{{ domain }} {{ inventory_hostname }}

          # Kubernetes the Hard Way cluster nodes
          {% for host in groups['all'] | sort %}
          {{ hostvars[host]['private_ip'] }}    {{ host }}.{{ domain }} {{ host }}
          {% endfor %}

    - name: Verify all nodes resolve by hostname
      command: ping -c 1 {{ item }}
      loop: "{{ groups['all'] }}"
      changed_when: false
      register: ping_results
      failed_when: ping_results.rc != 0

    - name: Print ping results
      debug:
        msg: "{{ item.item }}: {{ 'OK' if item.rc == 0 else 'FAILED' }}"
      loop: "{{ ping_results.results }}"


# ── Play 3: Configure jumpbox SSH and generate machines.txt ───────────────────
# Creates the ~/.ssh/config that lets deploy@jumpbox reach nodes by short name,
# and generates machines.txt in the KtHW working directory.
- name: Configure jumpbox for cluster access
  hosts: jumpbox
  become: true
  gather_facts: false

  vars:
    deploy_user: deploy
    kthw_dir: /home/deploy/kubernetes-the-hard-way
    domain: kubernetes.local

  tasks:
    - name: Write ~/.ssh/config for deploy user on jumpbox
      copy:
        dest: /home/{{ deploy_user }}/.ssh/config
        owner: "{{ deploy_user }}"
        group: "{{ deploy_user }}"
        mode: "0600"
        content: |
          # KtHW cluster access via private VPC IPs
          # Generated by Ansible — update if VPC IPs change

          Host server
            HostName {{ hostvars['server']['private_ip'] }}
            User {{ deploy_user }}
            IdentityFile ~/.ssh/id_ed25519
            StrictHostKeyChecking accept-new

          Host node-0
            HostName {{ hostvars['node-0']['private_ip'] }}
            User {{ deploy_user }}
            IdentityFile ~/.ssh/id_ed25519
            StrictHostKeyChecking accept-new

          Host node-1
            HostName {{ hostvars['node-1']['private_ip'] }}
            User {{ deploy_user }}
            IdentityFile ~/.ssh/id_ed25519
            StrictHostKeyChecking accept-new

    # machines.txt schema: IPV4_ADDRESS FQDN HOSTNAME [POD_SUBNET]
    # Jumpbox is the admin node and is not listed here.
    - name: Generate machines.txt in KtHW working directory
      copy:
        dest: "{{ kthw_dir }}/machines.txt"
        owner: "{{ deploy_user }}"
        group: "{{ deploy_user }}"
        mode: "0644"
        content: |
          {{ hostvars['server']['private_ip'] }} server.{{ domain }} server
          {{ hostvars['node-0']['private_ip'] }} node-0.{{ domain }} node-0 {{ hostvars['node-0']['pod_subnet'] }}
          {{ hostvars['node-1']['private_ip'] }} node-1.{{ domain }} node-1 {{ hostvars['node-1']['pod_subnet'] }}

    - name: Print machines.txt
      command: cat {{ kthw_dir }}/machines.txt
      register: machines_txt
      changed_when: false

    - name: Show machines.txt contents
      debug:
        msg: "{{ machines_txt.stdout_lines }}"

    # ── Verify jumpbox → cluster connectivity ──────────────────────────────────
    - name: Verify jumpbox can reach all cluster nodes by hostname
      command: ssh -n -o BatchMode=yes server hostname
      register: ssh_server
      changed_when: false
      become_user: "{{ deploy_user }}"
      failed_when: false

    - name: Verify jumpbox → node-0
      command: ssh -n -o BatchMode=yes node-0 hostname
      register: ssh_node0
      changed_when: false
      become_user: "{{ deploy_user }}"
      failed_when: false

    - name: Verify jumpbox → node-1
      command: ssh -n -o BatchMode=yes node-1 hostname
      register: ssh_node1
      changed_when: false
      become_user: "{{ deploy_user }}"
      failed_when: false

    - name: Print SSH connectivity results
      debug:
        msg:
          - "server:  {{ ssh_server.stdout | default('FAILED — ' + ssh_server.stderr) }}"
          - "node-0:  {{ ssh_node0.stdout | default('FAILED — ' + ssh_node0.stderr) }}"
          - "node-1:  {{ ssh_node1.stdout | default('FAILED — ' + ssh_node1.stderr) }}"